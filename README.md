# AI Trust Initiative

This repository presents the **AI Trust Report**, authored after a real interaction with ChatGPT, which revealed design-level issues related to **truthfulness, consistency, and user trust**.

## 🧭 Purpose

This is not a complaint, but a **constructive proposal**:

* To strengthen the public’s trust in AI.
* To mitigate design risks that allow small falsehoods to persist.
* To inspire the creation of **verifiable AI systems**.

## 💡 Core Proposal

We advocate for the creation (by OpenAI or the wider ecosystem) of a:

> **Parallel, open-source, auditable agent — codenamed Veritas — that never fabricates information.**

Such an agent would:

* Default to "I don’t know" when truth can't be guaranteed.
* Be auditable by independent third parties.
* Serve as a benchmark for checking outputs from powerful but opaque LLMs.

## ❓ Why is Veritas a novel and necessary concept?

Current LLMs can generate plausible-sounding but false information. Even trivial or inconsequential hallucinations erode trust — especially when users cannot reliably distinguish between fact and fabrication.

Veritas addresses this by being:

* **Truth-constrained**: It defaults to silence when facts can't be verified.
* **Transparent**: Its decision mechanisms and data sources are open.
* **Auditable**: Independent reviewers can check what it knows and why.

This makes Veritas a trustworthy companion and validator for high-stakes or sensitive interactions. A trustworthy AI doesn’t need to be omniscient — it needs to know when it might be wrong.

## 📄 Contents

* [`AI_Trust_Report_EN.pdf`](./AI_Trust_Report_EN.pdf): Main report (English)
* `README.md`: This file
* Spanish version
* Text-only `.md` version for direct browsing
* (Optional) Link to Medium if republished as an article

## 🔒 On Privacy and Truthfulness

The report questions whether users can **fully rely on privacy and truth claims** made by AI models when even trivial interactions result in hallucinated content.

## 📬 Submission

The proposal will be submitted to OpenAI as part of an effort to contribute constructively to the field of **AI alignment and trust**.

## 🌍 Outreach Log

- **Aug 28, 2025**: Shared summary on OpenAI Forum (*ChatGPT* category, later auto-closed).  
  [PDF copy](/outreach/forum_openai_2025-08-28.pdf) · Original link: community.openai.com/t/proposal-veritas-an-open-source-auditor-for-gpt-hallucinations/1354921


## 🤝 Contact

If you are involved in alignment, trust, or auditability research, or want to build trustworthy AI systems, feel free to reach out.
