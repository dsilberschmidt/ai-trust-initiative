# AI Trust Initiative

This repository presents the **AI Trust Report**, based on a real interaction with ChatGPT, which revealed design-level issues related to **truthfulness, consistency, and user trust**.

## ğŸ§­ Purpose

This is not a complaint, but a **constructive proposal**:

* To strengthen the publicâ€™s trust in AI.
* To mitigate design risks that allow small falsehoods to persist.
* To inspire the creation of **verifiable AI systems**.

## ğŸ’¡ Core Proposal

We advocate for the creation (by OpenAI or the wider ecosystem) of a:

> ### ğŸ›¡ï¸ Veritas
> A **parallel, open-source, auditable agent** that **never fabricates information**.

Such an agent would:

* Default to "I donâ€™t know" when truth can't be guaranteed.
* Be auditable by independent third parties.
* Serve as a benchmark for checking outputs from powerful but opaque LLMs.

## ğŸ“„ Contents

* [`AI_Trust_Report_EN.pdf`](./AI_Trust_Report_EN.pdf): Main report (English)
* `README.md`: This file
* Spanish version
* Text-only `.md` version for direct browsing
* (Optional) Link to Medium if republished as an article

## ğŸ”’ On Privacy and Truthfulness

The report questions whether users can **fully rely on privacy and truth claims** made by AI models when even trivial interactions result in hallucinated content.

## ğŸ“¬ Submission

The proposal will be submitted to Openai as part of an effort to contribute constructively to the field of **AI alignment and trust**.

## ğŸ¤ Contact

This initiative was created and documented by the owner of this repository. If you're involved in alignment, trust, or auditability research, feel free to reach out via GitHub.
