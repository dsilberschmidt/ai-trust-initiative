# AI Trust Report

This repository contains the **AI Trust Report**, a user-generated document exploring the limits of trust, privacy, and truthfulness in AI interactions.

## ğŸ“Œ Purpose

The report was written after an extended conversation with ChatGPT that evolved from a technical query into a broader reflection on how users experience transparency, reliability, and design inconsistencies in AI systems.

Its aim is not to attack but to **constructively highlight areas where trust may be eroded**, and how OpenAI (or any future developer) might address these issues.

## ğŸ“„ Contents

- `AI_Trust_Report_EN.pdf`: The full trust report (in English)
- `README.md`: This file
- (Optional) `AI_Trust_Report_ES.pdf`: Spanish version
- (Optional) `Medium_Link.md`: If later published as an article

## ğŸ“¬ Submission

This document will be sent to [alignment@openai.com](mailto:alignment@openai.com) as part of a broader proposal to support trust and audit mechanisms in AI systems.

## ğŸ’¡ Proposal

Among other things, the report proposes:
- A parallel, audited, open-source LLM agent that **never hallucinates** (just says â€œI donâ€™t knowâ€)
- A clearer distinction between confident and uncertain outputs
- A better way for users to track and escalate design issues

## ğŸ”’ About Privacy

The report raises critical questions about whether users can take AI privacy claims at face value, especially when minor inconsistencies or falsehoods are allowed in trivial domains.

## ğŸ¤ Contact

If this project interests you or you're involved in trust and alignment research, feel free to reach out.
